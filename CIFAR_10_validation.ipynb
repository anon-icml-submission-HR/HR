{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfc25f2",
   "metadata": {},
   "source": [
    "### This is the code reproducing the main experiments found in section. 6 of the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50f1cb",
   "metadata": {},
   "source": [
    "#### Pytorch Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcaf0fc",
   "metadata": {},
   "source": [
    "#### External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fdffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks # From Madry et. al, implementing popular adv. attack methods\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from pathlib import Path # For creating directories to save results\n",
    "import os # For setting environment variables (MOSEK license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a945674",
   "metadata": {},
   "source": [
    "#### Functions from within this repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pt_models.resnet import *\n",
    "from helper_functions import *\n",
    "from HR import * # The main file implementing HR and returning a PyTorch loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f77939",
   "metadata": {},
   "source": [
    "#### What you will need to practically run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MOSEKLM_LICENSE_FILE']=\"mosek.lic\" # Easily obtained via https://www.mosek.com/products/academic-licenses/\n",
    "torch.version.cuda == '11.3' # Change according to your PyTorch version (Cuda 11.3 is compatible with the latest PyTorch)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use a GPU if one is available, otherwise a CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82535c2b",
   "metadata": {},
   "source": [
    "#### CIFAR-10 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation_mean = [0.4914, 0.4822, 0.4465] # Commonly used normalization for CIFAR-10\n",
    "normalisation_std = [0.2023, 0.1994, 0.2010] # Converts images to [0, 1] scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3733aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(normalisation_mean, normalisation_std),\n",
    "]) # Applying pre-processing transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30662256",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(normalisation_mean, normalisation_std),\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac337e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 128 # Commonly used CIFAR-10 training batch size\n",
    "train_batches = 390 # Simply floor(50000/128). The number of complete training batches.\n",
    "test_batch_size = 100 # Testing batch size. Again commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=train_batch_size, shuffle=False, num_workers=20)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=test_batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d3b38",
   "metadata": {},
   "source": [
    "#### HR Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e0c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "α_choice = 0.1 # Trialled values were [0, 0.05, 0.1, 0.2]\n",
    "r_choice = 0.1 # Trialled values were [0, 0.05, 0.1, 0.2]\n",
    "ϵ_choice = 0.1 # Trialled values were [0, 0.05, 0.1, 0.2]\n",
    "# This gives 4x4x4 = 64 models in total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111abe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness specifications\n",
    "model_name = f\"alpha = {α_choice}, r = {r_choice}, eps = {ϵ_choice}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3425b",
   "metadata": {},
   "source": [
    "#### Poisining/Corruption sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bce8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error sources in the data\n",
    "frac = 0.25 # What fraction of the training data should we consider as a finite random sample?\n",
    "mis_specification = 0.1 # What fraction of the training labels are misspecified?\n",
    "noise = 0.1 # How much adversarial noise is in the data?\n",
    "gaussian_noise = 0.5 # How much gaussian noise is in the data? (Note that noise and gaussian noise are considered separately, not applied together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4582ae9",
   "metadata": {},
   "source": [
    "# Final training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in np.arange(0, 10): # Run 10 iterations using different samples\n",
    "    \n",
    "    torch.manual_seed(iter) # Seed will be the iteration number throughout\n",
    "    torch.cuda.manual_seed_all(iter)\n",
    "    torch.backends.cudnn.deterministic = True # Makes it reproducible\n",
    "    torch.backends.cudnn.benchmark = True \n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # Data & Training Specifications\n",
    "    epochs = 300\n",
    "    lr = 0.01\n",
    "    resume = False\n",
    "\n",
    "    start_epoch = 0 # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "    # Implementing the error sources.\n",
    "    # Statistical error - sample less than the full dataset\n",
    "    \n",
    "    # Function to corrupt the data. That is, to sample less than the full data size and corrupt some of the labels\n",
    "    # See the paper for more details.\n",
    "    sampled_batches, data_points_to_corrupt, unique_labels = return_batches_to_corrupt(iter, \n",
    "                                                                                       train_batches, \n",
    "                                                                                       train_batch_size, \n",
    "                                                                                       frac, \n",
    "                                                                                       mis_specification, \n",
    "                                                                                       num_classes)\n",
    "    \n",
    "    # Splitting the training batches into training and validation (70/30 split)\n",
    "    np.random.seed(iter)\n",
    "    training_batches = np.random.choice(sampled_batches, size = int(0.7*len(sampled_batches)), replace = False) \n",
    "    validation_batches = [i for i in sampled_batches if i not in training_batches]\n",
    "    \n",
    "    # Initialising the NN\n",
    "    net = ResNet18(iter)\n",
    "    net = net.to(device)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        net = torch.nn.DataParallel(net)\n",
    "\n",
    "    ########### TRAINING ##################\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialising Holistic Robustness\n",
    "    HR = HR_Neural_Networks(NN_model = net,\n",
    "                        learning_approach = \"HD\",\n",
    "                        train_batch_size = train_batch_size,\n",
    "                        loss_fn = criterion,\n",
    "                        normalisation_used = [normalisation_mean, normalisation_std],\n",
    "                        α_choice = α_choice, \n",
    "                        r_choice = r_choice,\n",
    "                        ϵ_choice = ϵ_choice,\n",
    "                        adversarial_steps=10,\n",
    "                        adversarial_step_size=0.2\n",
    "                        )\n",
    "\n",
    "    def train(epoch):\n",
    "\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss, HR_losses = [], []\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "\n",
    "            if batch_idx in training_batches: # Implements subsampling\n",
    " \n",
    "                targets = corrupt_targets(batch_idx,\n",
    "                targets,\n",
    "                training_batches,\n",
    "                train_batch_size,\n",
    "                train_batches,\n",
    "                data_points_to_corrupt,\n",
    "                unique_labels) # Corrupting labels \n",
    "\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # Clearing the gradient from the previous step\n",
    "            \n",
    "                if (α_choice != 0 or r_choice != 0 or ϵ_choice != 0):\n",
    "                    HR_loss = HR.HR_criterion(inputs, targets, device) # Resolving to find the new HR loss function\n",
    "                else:\n",
    "                    # If no robustness is specified, just train as normal.\n",
    "                    outputs = net(inputs)\n",
    "                    HR_loss = torch.sum(criterion(outputs, targets))/train_batch_size\n",
    "\n",
    "                # Backprop w.r.t HR loss\n",
    "                HR_loss.backward()\n",
    "                HR_losses.append(HR_loss.cpu().detach().numpy()) # Now saving it\n",
    "                \n",
    "                # We also collect regular, unweighted loss\n",
    "                outputs = net(inputs)\n",
    "                loss = torch.sum(criterion(outputs, targets))/train_batch_size \n",
    "                train_loss.append(loss.item())\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = outputs.max(1) # \\hat{y} = \\argmax{\\hat{p}}\n",
    "                total += targets.size(0) # Increment the total\n",
    "                correct += predicted.eq(targets).sum().item() # Increment correct tally if \\hat{y} is correct\n",
    "\n",
    "        training_accuracy = correct/total\n",
    "\n",
    "        # Collect unweighted loss, training accuracy and weighted HR loss.\n",
    "        return np.mean(train_loss), training_accuracy, np.mean(HR_losses)\n",
    "\n",
    "\n",
    "    ########### VALIDATION ##################\n",
    "\n",
    "    # Initialise validation/testing adversary\n",
    "    adversarial_attack_test = torchattacks.PGDL2(net,\n",
    "                                                 eps=noise,\n",
    "                                                 alpha=0.2,\n",
    "                                                 steps=10,\n",
    "                                                 random_start=True,\n",
    "                                                 eps_for_division=1e-10) \n",
    "\n",
    "    # Important to set the same normalization as the original images\n",
    "    adversarial_attack_test.set_normalization_used(\n",
    "        mean=normalisation_mean, std=normalisation_std)\n",
    "    \n",
    "    def validate(epoch):\n",
    "        net.eval()\n",
    "\n",
    "        # Metrics - loss. Adversarial data, natural data and (gaussian) noisy data\n",
    "        adv_test_loss, nat_test_loss, gn_test_loss= [], [], [] \n",
    "\n",
    "        # Metrics - accuracy. Adversarial data, natural data and (gaussian) noisy data\n",
    "        adv_correct, nat_correct, gn_correct, total = 0, 0, 0, 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            \n",
    "            if batch_idx in validation_batches:\n",
    "                \n",
    "                targets = corrupt_targets(batch_idx,\n",
    "                targets,\n",
    "                validation_batches,\n",
    "                train_batch_size,\n",
    "                train_batches, \n",
    "                data_points_to_corrupt,\n",
    "                unique_labels) # Corrupting labels\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                if noise > 0:\n",
    "                    adv = adversarial_attack_test(inputs, targets) # validation-time adv. attack\n",
    "\n",
    "                else:\n",
    "                    adv = inputs # If eps = 0, then there's no attack\n",
    "\n",
    "                if gaussian_noise > 0:\n",
    "                    # Adding gaussian noise to 3 channels of the CIFAR-10 image\n",
    "                    gn = gaussian_noise * torch.randn(*inputs.shape) \n",
    "                    gn = gn.to(device)\n",
    "                    gn_inputs = inputs + gn\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    if gaussian_noise > 0:\n",
    "                        # Evaluating on gaussian noise validation images\n",
    "                        gn_outputs = net(gn_inputs)\n",
    "                        gn_loss_vec = criterion(gn_outputs, targets)\n",
    "                        gn_loss = torch.sum(gn_loss_vec)\n",
    "                        gn_test_loss.append(gn_loss.item()/test_batch_size)\n",
    "                        gn_predictions = gn_outputs.max(1)[1]\n",
    "                        gn_correct += gn_predictions.eq(targets).sum().item()\n",
    "\n",
    "                    if noise > 0:\n",
    "                        # Evaluating on adversarial validation images\n",
    "                        adv_outputs = net(adv)\n",
    "                        adv_loss_vec = criterion(adv_outputs, targets)\n",
    "                        adv_loss = torch.sum(adv_loss_vec)\n",
    "                        adv_test_loss.append(adv_loss.item()/test_batch_size)\n",
    "                        adv_predictions = adv_outputs.max(1)[1]\n",
    "                        adv_correct += adv_predictions.eq(targets).sum().item()\n",
    "                        adv_outputs = net(adv)\n",
    "\n",
    "                    # Evaluating on natural validation images\n",
    "                    nat_outputs = net(inputs)\n",
    "                    nat_loss_vec = criterion(nat_outputs, targets)\n",
    "                    nat_loss = torch.sum(nat_loss_vec)\n",
    "                    nat_test_loss.append(nat_loss.item()/test_batch_size)\n",
    "                    nat_predictions = nat_outputs.max(1)[1]\n",
    "                    nat_correct += nat_predictions.eq(targets).sum().item()\n",
    "\n",
    "                    total += targets.size(0)\n",
    "\n",
    "        # Initialise saving of metrics\n",
    "        outputs = {\"Adv Val Loss\": -1, \"Adv Val Accuracy\": -1,\n",
    "                   \"GN Val Loss\": -1, \"GN Val Accuracy\": -1,\n",
    "                   \"Nat Val Loss\": -1, \"Nat Val Accuracy\": -1}\n",
    "\n",
    "        # Only save adversarial noise loss/accuracy if adv. noise is present\n",
    "        if noise > 0:\n",
    "\n",
    "            outputs[\"Adv Val Loss\"] = np.mean(adv_test_loss)\n",
    "            adv_accuracy = adv_correct/total\n",
    "            outputs[\"Adv Val Accuracy\"] = adv_accuracy\n",
    "\n",
    "\n",
    "        # Only save gaussian noise loss/accuracy if gaussian noise is present\n",
    "        if gaussian_noise > 0:\n",
    "\n",
    "            outputs[\"GN Val Loss\"] = np.mean(gn_test_loss)\n",
    "            gn_accuracy = gn_correct/total\n",
    "            outputs[\"GN Val Accuracy\"] = gn_accuracy\n",
    "\n",
    "        # Always collect natural loss/accuracy\n",
    "        nat_accuracy = nat_correct/total\n",
    "        outputs[\"Nat Val Loss\"] = np.mean(nat_test_loss)\n",
    "        outputs[\"Nat Val Accuracy\"] = nat_accuracy\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    ########### TESTING ##################\n",
    "\n",
    "    def test(epoch):\n",
    "        net.eval()\n",
    "\n",
    "        # Metrics - loss. Adversarial data, natural data and (gaussian) noisy data\n",
    "        adv_test_loss, nat_test_loss, gn_test_loss = [], [], []\n",
    "\n",
    "        # Metrics - accuracy. Adversarial data, natural data and (gaussian) noisy data\n",
    "        adv_correct, nat_correct, gn_correct, total = 0, 0, 0, 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if noise > 0:\n",
    "                adv = adversarial_attack_test(inputs, targets) # test-time adv. attack\n",
    "\n",
    "            else:\n",
    "                adv = inputs # If eps = 0, then there's no attack\n",
    "\n",
    "            if gaussian_noise > 0:\n",
    "                # Adding gaussian noise to 3 channels of the CIFAR-10 image\n",
    "                gn = gaussian_noise * torch.randn(*inputs.shape)\n",
    "                gn = gn.to(device)\n",
    "                gn_inputs = inputs + gn\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                if gaussian_noise > 0:\n",
    "\n",
    "                    # Evaluating on gaussian noise testing images\n",
    "                    gn_outputs = net(gn_inputs)\n",
    "                    gn_loss_vec = criterion(gn_outputs, targets)\n",
    "                    gn_loss = torch.sum(gn_loss_vec)\n",
    "                    gn_test_loss.append(gn_loss.item()/test_batch_size)\n",
    "                    gn_predictions = gn_outputs.max(1)[1]\n",
    "                    gn_correct += gn_predictions.eq(targets).sum().item()\n",
    "\n",
    "\n",
    "                if noise > 0:\n",
    "                    # Evaluating on adversarial testing images\n",
    "                    adv_outputs = net(adv)\n",
    "                    adv_loss_vec = criterion(adv_outputs, targets)\n",
    "                    adv_loss = torch.sum(adv_loss_vec)\n",
    "                    adv_test_loss.append(adv_loss.item()/test_batch_size)\n",
    "                    adv_predictions = adv_outputs.max(1)[1]\n",
    "                    adv_correct += adv_predictions.eq(targets).sum().item()\n",
    "                    adv_outputs = net(adv)\n",
    "\n",
    "                # Evaluating on natural testing images\n",
    "                nat_outputs = net(inputs)\n",
    "                nat_loss_vec = criterion(nat_outputs, targets)\n",
    "                nat_loss = torch.sum(nat_loss_vec)\n",
    "                nat_test_loss.append(nat_loss.item()/test_batch_size)\n",
    "                nat_predictions = nat_outputs.max(1)[1]\n",
    "                nat_correct += nat_predictions.eq(targets).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        # Initialise saving of metrics\n",
    "        outputs = {\"Adv Test Loss\": -1, \"Adv Test Accuracy\": -1,\n",
    "                   \"GN Test Loss\": -1, \"GN Test Accuracy\": -1,\n",
    "                   \"Nat Test Loss\": -1, \"Nat Test Accuracy\": -1}\n",
    "\n",
    "        # Only save adversarial noise loss/accuracy if adv. noise is present\n",
    "        if noise > 0:\n",
    "\n",
    "            outputs[\"Adv Test Loss\"] = np.mean(adv_test_loss)\n",
    "            adv_accuracy = adv_correct/total\n",
    "            outputs[\"Adv Test Accuracy\"] = adv_accuracy\n",
    "\n",
    "        # Only save gaussian noise loss/accuracy if gaussian noise is present\n",
    "        if gaussian_noise > 0:\n",
    "\n",
    "            outputs[\"GN Test Loss\"] = np.mean(gn_test_loss)\n",
    "            gn_accuracy = gn_correct/total\n",
    "            outputs[\"GN Test Accuracy\"] = gn_accuracy\n",
    "\n",
    "        # Always collect natural loss/accuracy\n",
    "        nat_accuracy = nat_correct/total\n",
    "        outputs[\"Nat Test Loss\"] = np.mean(nat_test_loss)\n",
    "        outputs[\"Nat Test Accuracy\"] = nat_accuracy\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # RUNNING TRAINING, VALIDATION AND TESTING\n",
    "\n",
    "    # Initialising loss metrics\n",
    "    train_losses, HR_losses, nat_val_losses, adv_val_losses, gn_val_losses, nat_test_losses, adv_test_losses, gn_test_losses = [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # Initialising accuracy metrics\n",
    "    train_accuracies, nat_val_accuracies, adv_val_accuracies, gn_val_accuracies, nat_test_accuracies, adv_test_accuracies, gn_test_accuracies = [], [], [], [], [], []\n",
    "\n",
    "    # Iterator for stopping criterion - we will run until training loss has converged\n",
    "    stopping_criterion = 0\n",
    "\n",
    "    min_epochs = 220\n",
    "    end_epoch = start_epoch+epochs\n",
    "\n",
    "    # Final training loop\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "\n",
    "        # If the stopping criterion has become  or we've reached the final number, STOP.\n",
    "        stop = (stopping_criterion >= 6 and epoch >= min_epochs) or epoch == end_epoch-1 \n",
    "\n",
    "        train_loss, train_accuracy, HR_loss = train(epoch) # Training at every epoch\n",
    "\n",
    "        if epoch % 20 == 0 or stop: # Collect metrics every 20 epochs or when the training is over\n",
    "            \n",
    "            # Training metrics\n",
    "            train_losses.append(train_loss)\n",
    "            HR_losses.append(HR_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "        \n",
    "            # Running validation. Note this is to save computation time as this step will run only every 20 epochs\n",
    "            val_metrics = validate(\n",
    "                epoch)\n",
    "            \n",
    "            # Validation metrics\n",
    "            adv_val_losses.append(val_metrics[\"Adv Val Loss\"])\n",
    "            nat_val_losses.append(val_metrics[\"Nat Val Loss\"])\n",
    "            gn_val_losses.append(val_metrics[\"GN Val Loss\"])\n",
    "            adv_val_accuracies.append(val_metrics[\"Adv Val Accuracy\"])\n",
    "            nat_val_accuracies.append(val_metrics[\"Nat Val Accuracy\"])\n",
    "            gn_val_accuracies.append(val_metrics[\"GN Val Accuracy\"])\n",
    "            \n",
    "            # Running testing. Note this is to save computation time as this step will run only every 20 epochs.\n",
    "            test_metrics = test(\n",
    "                epoch)\n",
    "\n",
    "            # Testing metrics                 \n",
    "            adv_test_losses.append(test_metrics[\"Adv Test Loss\"])\n",
    "            nat_test_losses.append(test_metrics[\"Nat Test Loss\"])\n",
    "            gn_test_losses.append(test_metrics[\"GN Test Loss\"])\n",
    "            adv_test_accuracies.append(test_metrics[\"Adv Test Accuracy\"])\n",
    "            nat_test_accuracies.append(test_metrics[\"Nat Test Accuracy\"])\n",
    "            gn_test_accuracies.append(test_metrics[\"GN Test Accuracy\"])\n",
    "        \n",
    "        # Stopping criterion - is the current training loss higher than the minimum achieved so far? (We are aiming to train to convergence)\n",
    "        best_train_loss = min(train_losses)\n",
    "\n",
    "        if train_loss > best_train_loss:\n",
    "            stopping_criterion += 1\n",
    "        else:\n",
    "            stopping_criterion = 0\n",
    "\n",
    "        # If stopping criterion is reached, then save models and metrics.\n",
    "        if stop:\n",
    "\n",
    "            print('Saving..')\n",
    "            \n",
    "            models_path = f\"HD_models/{model_name}/frac_misspecified_{mis_specification}/frac_data_{frac}/iter_{iter}/\"\n",
    "            Path(models_path).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(net.state_dict(), models_path + \"checkpoint.pt\")\n",
    "\n",
    "            losses = pd.DataFrame([train_losses, HR_losses, \n",
    "                                   nat_val_losses, adv_val_losses, gn_val_losses,\n",
    "                                   nat_test_losses, adv_test_losses, gn_test_losses,\n",
    "                                   train_accuracies, \n",
    "                                   nat_val_accuracies, adv_val_accuracies, gn_test_accuracies,\n",
    "                                   nat_test_accuracies, adv_test_accuracies, gn_test_accuracies]).T\n",
    "                                        \n",
    "            losses.columns = [\"Training Loss\", \"Inflated Loss\", \n",
    "                              \"Natural Validation Loss\", \"Adversarial Validation Loss\", \"Gaussian Noise Validation Loss\",\n",
    "                              \"Natural Testing Loss\", \"Adversarial Testing Loss\", \"Gaussian Noise Testing Loss\",\n",
    "                              \"Training Accuracy\", \n",
    "                              \"Natural Validation Accuracy\", \"Adversarial Testing Accuracy\", \"Gaussian Noise Testing Accuracy\",\n",
    "                              \"Natural Testing Accuracy\", \"Adversarial Testing Accuracy\", \"Gaussian Noise Testing Accuracy\"]\n",
    "            \n",
    "            results_path = f\"HD_results/{model_name}/frac_misspecified_{mis_specification}/frac_data_{frac}/iter_{iter}/\"\n",
    "            Path(results_path).mkdir(parents=True, exist_ok=True)\n",
    "            losses.to_csv(results_path + \"metrics.csv\")\n",
    "            \n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
